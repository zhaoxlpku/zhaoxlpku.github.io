---
title: "Zero-Resource Knowledge-Grounded Dialogue Generation"
collection: publications
category: conferences
permalink: /publication/2020-neurips-zero
date: 2020-12-06
venue: 'NeurIPS'
location: Online
author: Linxiao Li,Can Xu,Wei Wu,YUFAN ZHAO,Xueliang Zhao,Chongyang Tao
cofirst: 0
url: 'https://proceedings.neurips.cc/paper_files/paper/2020/file/609c5e5089a9aa967232aba2a4d03114-Paper.pdf'
---

While neural conversation models have shown great potentials towards generating informative and engaging responses via introducing external knowledge, learning such a model often requires knowledge-grounded dialogues that are difficult to obtain. To overcome the data challenge and reduce the cost of building a knowledge-grounded dialogue system, we explore the problem under a zero-resource setting by assuming no context-knowledge-response triples are needed for training. To this end, we propose representing the knowledge that bridges a context and a response and the way that the knowledge is expressed as latent variables, and devise a variational approach that can effectively estimate a generation model from independent dialogue corpora and knowledge corpora. Evaluation results on three benchmarks of knowledge-grounded dialogue generation indicate that our model can achieve comparable performance with state-of-the-art methods that rely on knowledge-grounded dialogues for training, and exhibits a good generalization ability over different datasets.